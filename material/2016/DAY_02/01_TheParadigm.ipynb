{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MapReduce\n",
    "\n",
    "A different paradigm for processing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We have some data\n",
    "<img src=\"http://www.gilliganondata.com/wp-content/uploads/2012/06/ExcelTables_1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "> What does it mean by “sharding” a set or database?\n",
    "\n",
    "* Partition the database into smaller parts by using a column-based selection \n",
    "    - **Vertical partitioning**\n",
    "* Partition the database into smaller parts by using a row-based selection \n",
    "    - **Horizontal partitioning**\n",
    "* Partition the database into smaller parts by using a procedure\n",
    "    - e.g. standard hash function: `k(x) -> n(x * p)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are many systems that take advantage \n",
    "\n",
    "of partinioning in computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## **MapReduce** is a completely different paradigm\n",
    "\n",
    "\n",
    "* Solving a certain subset of parallelizable problems\n",
    "* It brings the compute to the data\n",
    "* Input data is not stored on a separate storage system\n",
    "    * Data exists in little pieces and is permanently stored on each computing node\n",
    "    * distributed system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## History of implementations of the functional programming approach \n",
    "## on a distributed system\n",
    "\n",
    "* Google File System (paper published in 2003)\n",
    "* Google MapReduce (paper published in 2003 – implemented at Google in 2002)\n",
    "* Hadoop (2006-2008) \n",
    "    - HDFS\n",
    "    - MapReduce\n",
    "    - A whole ecosystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## GFS design\n",
    "\n",
    "* Hardware failures are common\n",
    "    - If medium-time-between-failure is 1 year – Then 10000 servers have one failure / hour\n",
    "* Files are huge (GB) \n",
    "* Sequential writes\n",
    "    - typically most files are mutated by appending newdata rather than overwriting\n",
    "* Sequential reads\n",
    "    - once written, the files are only read, and often only sequentially\n",
    "* High sustained bandwidth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Sharding/Chunking\n",
    "\n",
    "- Files are divided into fixed-size chunks\n",
    "- Size: typically 64/128 MB (modifiable parameter)\n",
    "- Files are replicated (by default 3 times, fault-tolerance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Data replication\n",
    "\n",
    "What’s the reason why a new data set gets replicated three times of different nodes?\n",
    "\n",
    "- Performance\n",
    "    * It improves the data locality\n",
    "- Fault-tolerance\n",
    "    * If a node crashes, its load can be moved onto another resource\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Advantages of (large) fixed-size chunks:\n",
    "  * Disk seek time small compared to transfer time\n",
    "  * A single file can be larger than a node’s disk space   \n",
    "  * Fixed size makes allocation computations easy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Why not increase the chunk size further?\n",
    "\n",
    "- Maps task operate on one chunk at a time\n",
    "- the increasing of the chunk size decreases the parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Write Operation\n",
    "\n",
    "> If some replica returns a failure, \n",
    "*the offset value* is changed and the write process is restarted with the new value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"https://www.cs.rutgers.edu/~pxk/417/notes/images/dfs-1-gfs-chunks.png\" width=800>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hadoop\n",
    "(and Hadoop streaming)\n",
    "\n",
    "<center>\n",
    "<img src='http://www.opensourceforu.efytimes.com/wp-content/uploads/2012/03/hadoop-database-590x321.jpg'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Hadoop distributed file system\n",
    "\n",
    "HDFS stores large files (typically in the range of gigabytes to terabytes) across multiple machines. \n",
    "\n",
    "It achieves reliability by replicating the data across multiple hosts, and hence theoretically does not require RAID storage on hosts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A Hadoop cluster has nominally a single namenode plus a cluster of datanodes\n",
    "\n",
    "- Each datanode serves up blocks of data over the network using a block protocol specific to HDFS. \n",
    "- The file system uses TCP/IP sockets for communication. \n",
    "- Clients use remote procedure call (RPC) to communicate between each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What you receive from an Hadoop environment\n",
    "\n",
    "* Partitioning the input data\n",
    "* Scheduling the program’s execution across a set of machines\n",
    "* Performing the group by key step\n",
    "* Handling node failures\n",
    "* Managing required inter-machine communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> Ok. What is MapReduce again?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Map function\n",
    "\n",
    "processes data and generates a set of intermediate key/value pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Reduce function\n",
    "\n",
    "merges all intermediate values associated with the same intermediate key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"images/mapreduce.png\" width=600>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word count\n",
    "The '`Hello World`' for MapReduce (with *Java*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Among the simplest of full Hadoop jobs you can run\n",
    "\n",
    "<img src='http://www.glennklockwood.com/data-intensive/hadoop/wordcount-schematic.png'\n",
    "width='700'>\n",
    "<small>Reading ***Moby Dick*** </small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Consider doing a word count of the following file using  MapReduce:\n",
    "```\n",
    "Hello World Bye World\n",
    "Hello Hadoop Goodbye Hadoop\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The map function reads in words one at a time outputs (“word”, 1) for each parsed input word\n",
    "\n",
    "```\n",
    "(Hello, 1)\n",
    "(World, 1)\n",
    "(Bye, 1)\n",
    "(World, 1)\n",
    "(Hello, 1)\n",
    "(Hadoop, 1)\n",
    "(Goodbye, 1)\n",
    "(Hadoop, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The shuffle phase between map and reduce creates a  list of values associated with each key\n",
    "```\n",
    "(Bye, (1))\n",
    "(Goodbye, (1))\n",
    "(Hadoop, (1, 1))\n",
    "(Hello, (1, 1))\n",
    "(World, (1, 1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The reduce function sums the numbers in the list for each  key and outputs (word, count) pairs\n",
    "```\n",
    "(Bye, 1)\n",
    "(Goodbye, 1)\n",
    "(Hadoop, 2)\n",
    "(Hello, 2)\n",
    "(World, 2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How can you do this with Java?\n",
    "(the Hadoop framework native language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "``` Java\n",
    "// Imports\n",
    "package org.myorg;\n",
    "import java.io.IOException;\n",
    "import java.util.*;\n",
    "import org.apache.hadoop.*\n",
    "\n",
    "// Create JAVA class\n",
    "public class WordCount {\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "``` Java\n",
    "//Mapper function\n",
    "  public static class Map extends MapReduceBase implements Mapper<LongWritable, Text, Text, IntWritable> {\n",
    "    private final static IntWritable one = new IntWritable(1);\n",
    "    private Text word = new Text();\n",
    "\n",
    "    public void map(LongWritable key, Text value, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException {\n",
    "      String line = value.toString();\n",
    "      StringTokenizer tokenizer = new StringTokenizer(line);\n",
    "      while (tokenizer.hasMoreTokens()) {\n",
    "        word.set(tokenizer.nextToken());\n",
    "        output.collect(word, one);\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "``` Java\n",
    "//Reducer function\n",
    "  public static class Reduce extends MapReduceBase implements Reducer<Text, IntWritable, Text, IntWritable> {\n",
    "    public void reduce(Text key, Iterator<IntWritable> values, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException {\n",
    "      int sum = 0;\n",
    "      while (values.hasNext()) {\n",
    "        sum += values.next().get();\n",
    "      }\n",
    "      output.collect(key, new IntWritable(sum));\n",
    "    }\n",
    "  }\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<small>\n",
    "``` Java\n",
    "//Main function\n",
    "  public static void main(String[] args) throws Exception {\n",
    "    JobConf conf = new JobConf(WordCount.class);\n",
    "    conf.setJobName(\"wordcount\");\n",
    "\n",
    "    conf.setOutputKeyClass(Text.class);\n",
    "    conf.setOutputValueClass(IntWritable.class);\n",
    "\n",
    "    conf.setMapperClass(Map.class);\n",
    "    conf.setCombinerClass(Reduce.class);\n",
    "    conf.setReducerClass(Reduce.class);\n",
    "\n",
    "    conf.setInputFormat(TextInputFormat.class);\n",
    "    conf.setOutputFormat(TextOutputFormat.class);\n",
    "\n",
    "    FileInputFormat.setInputPaths(conf, new Path(args[0]));\n",
    "    FileOutputFormat.setOutputPath(conf, new Path(args[1]));\n",
    "\n",
    "    JobClient.runJob(conf);\n",
    "  }\n",
    "```\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can test the Java code here. *Live*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HADOOP_EXAMPLES=/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar\n"
     ]
    }
   ],
   "source": [
    "%env HADOOP_EXAMPLES /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  aggregatewordcount: An Aggregate based map/reduce program that counts the words in the input files.\r\n",
      "  aggregatewordhist: An Aggregate based map/reduce program that computes the histogram of the words in the input files.\r\n",
      "  multifilewc: A job that counts words from several files.\r\n",
      "  wordcount: A map/reduce program that counts the words in the input files.\r\n",
      "  wordmean: A map/reduce program that counts the average length of the words in the input files.\r\n",
      "  wordmedian: A map/reduce program that counts the median length of the words in the input files.\r\n",
      "  wordstandarddeviation: A map/reduce program that counts the standard deviation of the length of the words in the input files.\r\n"
     ]
    }
   ],
   "source": [
    "# Hadoop available examples\n",
    "! hadoop jar $HADOOP_EXAMPLES | grep word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: wordcount <in> [<in>...] <out>\r\n"
     ]
    }
   ],
   "source": [
    "# Check wordcount\n",
    "! hadoop jar $HADOOP_EXAMPLES wordcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Demo\n",
    "\n",
    "Note to my self: run next cells 'live'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%env HADOOP_EXAMPLES /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Our input\n",
    "! cat ../data/books/twolines.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "########################\n",
    "# Preprocess with HDFS\n",
    "\n",
    "# Create input directory\n",
    "hdfs dfs -mkdir myinput\n",
    "# Save one file inside\n",
    "file=\"../data/books/twolines.txt\"\n",
    "hdfs dfs -put $file myinput/file01\n",
    "# Remove output or Hadoop will give error if existing\n",
    "hdfs dfs -rm -r -f myoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Test wordcount with real hadoop on our system\n",
    "! hadoop jar $HADOOP_EXAMPLES wordcount myinput myoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Hadoop output\n",
    "! hadoop fs -cat myoutput/part*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hadoop streaming\n",
    "### Concepts and mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hadoop streaming is a utility \n",
    "\n",
    "* It comes bundled with the Hadoop distribution\n",
    "* It allows creating and running Map/Reduce jobs \n",
    "    - with any executable or script as the mapper and/or the reducer\n",
    "\n",
    "Protocol steps\n",
    "\n",
    "* Create a Map/Reduce job\n",
    "* Submit the job to an appropriate cluster\n",
    "* Monitor the progress of the job until it completes\n",
    "* Links to Hadoop HDFS job directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why?\n",
    "\n",
    "One of the most unappetizing aspects of Hadoop to users of traditional HPC is that it is written in Java. \n",
    "\n",
    "* Java is not originally designed to be a high-performance language\n",
    "* Learning Java is very difficult for domain scientists\n",
    "\n",
    "This is why Hadoop allows you to write map/reduce code in any language you want using the Hadoop Streaming interface\n",
    "\n",
    "* It means turning an existing Python or Perl script into a Hadoop job\n",
    "* Does not require learning any Java at all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A streaming command line example **for python**:\n",
    "``` bash\n",
    "$ hadoop jar $HADOOP_HOME/hadoop-streaming.jar \\\n",
    "    -files mapper.py,reducer.py\n",
    "    -input input_dir/ \\\n",
    "    -output output_dir/ \\\n",
    "    -mapper mapper.py \\\n",
    "    -reducer reducer.py \\\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Before submitting the Hadoop Streaming job:\n",
    "\n",
    "* Make sure your scripts have no errors\n",
    "* Do mapper and reducer scripts actually work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is just a matter of running them through pipes on a **little bit** of sample data,\n",
    "\n",
    "like `cat` or `head` linux bash commands, with pipes, as seen before.\n",
    "\n",
    "```\n",
    "# Simulating hadoop streaming with bash pipes\n",
    "$ cat $file | python mapper.py | sort | python reducer.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "TAB = \"\\t\"\n",
    "SEP = ':'\n",
    "import sys\n",
    "\n",
    "# Cycle current streaming data\n",
    "for line in sys.stdin:\n",
    "    # Clean input\n",
    "    line = line.strip()\n",
    "    # Skip SAM/BAM headers\n",
    "    if line[0] == \"@\":\n",
    "        continue\n",
    "    \n",
    "    # Use data\n",
    "    pieces = line.split(TAB)\n",
    "    mychr = pieces[2]\n",
    "    mystart = int(pieces[3])\n",
    "    myseq = pieces[9]\n",
    "\n",
    "    mystop = mystart + len(myseq)\n",
    "\n",
    "    # Each element with coverage\n",
    "    for i in range(mystart,mystop):\n",
    "        results = [mychr+SEP+i.__str__(), \"1\"]\n",
    "        print(TAB.join(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "TAB = \"\\t\"\n",
    "SEP = ':'\n",
    "import sys\n",
    "last_value = \"\"\n",
    "value_count = 1\n",
    "for line in sys.stdin:\n",
    "    value, count = line.strip().split(TAB)\n",
    "    # if this is the first iteration\n",
    "    if not last_value:\n",
    "        last_value = value\n",
    "    # if they're the same, log it\n",
    "    if value == last_value:\n",
    "        value_count += int(count)\n",
    "    else:\n",
    "        # state change\n",
    "        try: \n",
    "            print(TAB.join([last_value, str(value_count)]))\n",
    "        except:\n",
    "            pass\n",
    "        last_value = value\n",
    "        value_count = 1\n",
    "# LAST ONE after all records have been received\n",
    "print(TAB.join([last_value, str(value_count)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr1:10000\t3\n",
      "chr1:10001\t2\n",
      "chr1:10002\t2\n",
      "chr1:10003\t2\n",
      "chr1:10004\t2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "real\t0m3.880s\n",
      "user\t0m3.800s\n",
      "sys\t0m0.070s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# needs ~ 5 seconds for running\n",
    "time head -n 10000 $myfile | python mapper.py | sort | python reducer.py | head -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<big>\n",
    "A working python code tested on pipes **should work** with Hadoop Streaming\n",
    "</big>\n",
    "\n",
    "* To make this happen we need to handle copy of input and output files inside the Hadoop FS\n",
    "* Also the job tracker logs will be found inside HDFS\n",
    "* We are going to use bash scripting inside the notebook to make our workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "HDFS commands to interact with Hadoop file system use the same syntax:\n",
    "\n",
    "```\n",
    "hdfs dfs -command\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`command` are like bash commands for file\n",
    "\n",
    "e.g.\n",
    "\n",
    "```\n",
    "hadoop fs -mkdir hdfs:///dir\n",
    "hadoop fs -put file_on_host hdfs:///path/to/file\n",
    "hadoop fs -ls\n",
    "```\n",
    "\n",
    "<small>\n",
    "Note: we have seen this in action with the Java example\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<big>\n",
    "Hadoop Streaming needs “binaries” to execute\n",
    "</big>\n",
    "\n",
    "You need to specify interpreter at the beginning of your scripts:\n",
    "```\n",
    "#!/usr/bin/env python\n",
    "```\n",
    "\n",
    "Make also the script executables:\n",
    "```\n",
    "chmod +x hs*.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "! chmod +x mapper.py reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HADOOP_STREAMING=/usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar\n"
     ]
    }
   ],
   "source": [
    "%env HADOOP_STREAMING /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: $HADOOP_PREFIX/bin/hadoop jar hadoop-streaming.jar [options]\n",
      "Options:\n",
      "  dumptb <glob-pattern> Dumps all files that match the given pattern to \n",
      "                        standard output as typed bytes.\n",
      "  loadtb <path> Reads typed bytes from standard input and stores them in\n",
      "                a sequence file in the specified path\n",
      "  [streamjob] <args> Runs streaming job with given arguments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No Arguments Given!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Launch streaming\n",
    "hadoop jar $HADOOP_STREAMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted myinput\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16/03/15 17:04:11 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Preprocess with HDFS\n",
    "hdfs dfs -rm -r -f myinput\n",
    "hdfs dfs -mkdir myinput\n",
    "# Save one file inside\n",
    "hdfs dfs -put /tmp/ngs.sam myinput/file01\n",
    "# Remove output or Hadoop will give error if existing\n",
    "hdfs dfs -rm -r -f myoutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Final launch via bash command for using Hadoop streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "# A real Hadoop Streaming run\n",
    "time hadoop jar $HADOOP_STREAMING \\\n",
    "    -files mapper.py,reducer.py \\\n",
    "    -input myinput -output myoutput \\\n",
    "    -mapper mapper.py -reducer reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hadoop streaming is **difficult to debug**.\n",
    "Just like real Java Hadoop.\n",
    "\n",
    "If you did a typical setup mistake, you may end receiving unrelated errors stacktrace from the Java virtual machine.\n",
    "\n",
    "So before googling those stacktrace, make sure that:\n",
    "\n",
    "* Python files (mapper and reducer) exists\n",
    "* They are provided inside the main bash command also as **files** list\n",
    "* They are executables and contain as first line the hashbang\n",
    "* Your input directory exists on HDFS\n",
    "* Files inside your input directory are not corrupted\n",
    "    - e.g. bad decompression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CAP theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Consistency\n",
    "    - all nodes see the same data at the same time\n",
    "* Availability \n",
    "    - a guarantee that every request receives a response about whether it succeeded or failed\n",
    "* Partition tolerance \n",
    "    - the system continues to operate despite arbitrary partitioning due to network failures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"http://j.mp/1S7i4Eo\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# End of Chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paolo D. 2016-04-12 \n",
      "\n",
      "CPython 3.5.1\n",
      "IPython 4.1.2\n",
      "\n",
      "jupyter 1.0.0\n",
      "\n",
      "compiler   : GCC 4.4.7 20120313 (Red Hat 4.4.7-1)\n",
      "system     : Linux\n",
      "release    : 4.3.3-dhyve\n",
      "machine    : x86_64\n",
      "processor  : \n",
      "CPU cores  : 1\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a \"Paolo D.\" -d -v -m -p jupyter"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
