library(arules)
library(slam)
#############################################################################
#################### set working directory  #################################
#############################################################################
setwd("/pico/home/userinternal/msartori/Documents/CORSO_MILANO/DATA/POLIMI/RESULTS")
#############################################################################
############# loading polarity and stopwords ################################
#############################################################################
polarity=read.csv("/pico/home/userinternal/msartori/Documents/CORSO_MILANO/DATA/POLIMI/polarity.csv",header=T)
polarity[,1]=as.character(polarity[,1])
polarity=unique(polarity)
stopwords=read.table("/pico/home/userinternal/msartori/Documents/CORSO_MILANO/DATA/POLIMI/Stopwords_new.txt",header=F)
stopwords=stopwords[,1]
stopwords=unique(tolower(as.character(stopwords)))
#######################################################################################
##### reading and managing the data ###################################################
#######################################################################################
data <- read.table("/pico/home/userinternal/msartori/Documents/CORSO_MILANO/DATA/POLIMI/polimi.txt",header=T,comment.char="",quote="",sep="\t") # class: data.frame
colnames(data)
data=data[-43023,] ### tolgo il tweet bastardo
data=data[which(as.character(data$motivo_estrazione)!="la statale"),]
data=data[duplicated(data[,c(1,18)])==F,]
data<-as.data.frame (sapply(data, function(x) iconv(x, from='LATIN1',to='UTF8', sub='byte')))
data$text=as.character(data$text)
#################################################
### renaming motivo_estrazione ##################
#################################################
Uni=read.table("/pico/home/userinternal/msartori/Documents/CORSO_MILANO/DATA/POLIMI/UNIVERSITIES.txt",sep=";")
colnames(Uni)=c("from","to")
Uni=as.data.frame(sapply(Uni, function(x) iconv(x,from="LATIN1",to="UTF8", sub='byte')))
Uni$from=as.character(Uni$from)
Uni$to=as.character(Uni$to)
cambia=function(motivo,newMotivo=Uni){
if(motivo %in% newMotivo[,1]) to=newMotivo[which(newMotivo[,1]==motivo),2]
else to=motivo
return(to)
}
data$motivo_estrazione=(apply(as.matrix(data$motivo_estrazione),1,FUN=cambia))
data$motivo_estrazione=(apply(as.matrix(data$motivo_estrazione),1,FUN=cambia))
unique(data$motivo_estrazione)
table(is.na(data$motivo_estrazione))
#################################################
### keeping only UNIBO tweets ###################
#################################################
data=data[which(as.character(data$motivo_estrazione)=="UniboMagazine"),]
###general_infos -->> dropping the retweets!!
general_infos=data[-which(substr(data$text,1,2)=="RT"),c(2,19,21)]
colnames(general_infos)=c("ID","motivo","isUtente")
general_infos$ID=as.character(general_infos$ID)
general_infos$motivo=as.character(general_infos$motivo)
general_infos$isUtente=as.character(general_infos$isUtente)
data=data[-which(substr(data$text,1,2)=="RT"),c(2,1)]
colnames(data)=c("ID","MESSAGE")
ID=as.character(data$ID)
#############################################
#################### removing https #########
#############################################
mod=which(grepl("http",data$MESSAGE))
data$MESSAGE[mod]=gsub("http(s?)(:?)(/?)(/?)\\S*","",data$MESSAGE[mod])
rm(mod)
####################################################################################################
############# adding FIVE hashes before the IDs in order to keep track of them #####################
####################################################################################################
hash=function(x) paste("#####",x,sep="")
data$ID=apply(as.data.frame(data$ID),1,hash)
###########################################################################################
########### treeTagger  and grep (it is possible only in PICO environment) ################
###########################################################################################
write.table(data, file="reducedData.txt",row.names=F,col.names=F)
system("tree-tagger-italian reducedData.txt > treeTagger_output.txt")
# da fare se si ha bisogno del grep
#system('grep -v "^<!" treeTagger_output.txt > treeTaggerGrep.txt')
#taggedResults=read.table("treeTaggerGrep.txt",sep="\t",comment.char="",quote="",header=F,na.strings = "")
taggedResults=read.table("treeTagger_output.txt",sep="\t",comment.char="",quote="",header=F,na.strings = "")
system("rm reducedData.txt")
#system("rm treeTagger_output.txt")
############################################################################################################
################## keeping only the rows of taggedResults that contain useful types (SENTIMENT STEP) #######
############################################################################################################
tagCorrect=c("NOM","VER","ADJ","ADV")
keep=which((substr(as.character(taggedResults[,2]),1,3) %in% tagCorrect))
taggedResults=taggedResults[keep,]
rm(keep)
#######################################################
################## assigning an ID to each word #######
#######################################################
where_ID=which(substr(as.character(taggedResults[,1]),1,5)=="#####")
temp=c(where_ID[-1],(nrow(taggedResults)+1))
ID_length=temp-where_ID
taggedResults$ID=rep(substr(data$ID,6,nchar(data$ID)),times=ID_length)
#########################################################
######################## handling the data frame ########
#########################################################
taggedResults=taggedResults[-where_ID,c(4,1,3,2)]
colnames(taggedResults)=c("ID","parola","lemma","tipo")
taggedResults$lemma=as.character(taggedResults$lemma)
taggedResults$tipo=as.character(taggedResults$tipo)
taggedResults$ID=as.factor(taggedResults$ID)
#######################################################################
############################## resolving the hashtags and @s ##########
#######################################################################
lemma=function(x) {
if (x[3]=="<unknown>" &  substr(x[2],1,1)=="#" ) lemma_new=x[2]
else lemma_new=x[3]
return(tolower(lemma_new))
}
lemma_new=apply(X=taggedResults,FUN=lemma,MARGIN=1)
taggedResults$lemma=lemma_new
taggedResults=taggedResults[,-2]
#########################################################################################
################## keeping only the rows that are not null and not unknown ##############
#########################################################################################
discard=(taggedResults$lemma=="" | nchar(taggedResults$lemma)==1 | taggedResults$lemma=="<unknown>" | (taggedResults$lemma %in% stopwords) )
taggedResults=taggedResults[discard==FALSE,]
rm(discard,ID_length,temp,where_ID,lemma_new)
######################################
############# renaming the verbs #####
######################################
taggedResults$tipo[(substr(taggedResults$tipo,1,3)=="VER")]="VER"
#####################################################################################
################### SENTIMENT STEP ##################################################
#####################################################################################
taggedResults$SENT=0
taggedResults$MATCH=0
sent_pos=which(taggedResults$lemma %in% polarity$word)
SENT_DICT=function(word,dictionary=polarity){
pol=dictionary[which(dictionary[,1]==word),2]
match=abs(dictionary[which(dictionary[,1]==word),2])
if(length(pol)==0){
pol=0
match=0
}
out=list(pol=pol,match=match)
return(out)
}
taggedResults$SENT[sent_pos]=unlist(sapply(taggedResults$lemma[sent_pos],SENT_DICT)[1,])
taggedResults$MATCH[sent_pos]=unlist(sapply(taggedResults$lemma[sent_pos],SENT_DICT)[2,])
sentiment=(by(data=taggedResults$SENT,INDICES = as.character(taggedResults$ID),FUN=sum))
matched_words=(by(data=taggedResults$MATCH,INDICES = as.character(taggedResults$ID),FUN=sum))
ID_sentiment=names(sentiment)
sentiment=data.frame(as.numeric(sentiment),as.numeric(matched_words))
sentiment$ID=ID_sentiment
colnames(sentiment)=c("sentiment","matched_words","ID")
#####################################################
#### returning to the original IDs without hashes ###
#####################################################
data$ID=ID
#######################################################################
################# merging the sentiment with the data #################
#######################################################################
data=merge(data,sentiment)
#############################################################
############### setting the null polarities to NA ###########
#############################################################
data$sentiment[which(data$matched_words==0)]=NA
table((data$sentiment))
table(is.na(data$sentiment))
### other instructions and checks
taggedResults=taggedResults[,-c(4,5)]
which(!(unique(taggedResults$ID) %in% data$ID))
rm(ID_sentiment,sent_pos,tagCorrect,sentiment,polarity,matched_words,stopwords)
####################################################################################
############# removing everything that is not ADJ and NOM (clustering step) ########
####################################################################################
tagCorrect=c("NOM","ADJ")
keep=which((substr(as.character(taggedResults$tipo),1,3) %in% tagCorrect))
taggedResults=taggedResults[keep,]
rm(keep,tagCorrect)
########################################################################################
######################## saving the .load brefore the PERL script ######################
########################################################################################
write.table(taggedResults,row.names=F,col.names=F,"taggedData.load",sep="\t")
###############################################################################################
#################### PERL in order to create a file that is compatible with CLUTO #############
###############################################################################################
system("perl /pico/home/userinternal/msartori/Documents/CORSO_MILANO/CODE/PERL/info2cluto.pl < taggedData.load")
system("cat mio.firstrow mio.mat > cluto_matrix.mat")
system("mv mio.cname wordNames.txt")
system("mv mio.tran IDs.txt")
system("rm mio.firstrow mio.mat treeTagger_output.txt")
####################################################################
############################ CLUTO CLUSTERING ######################
####################################################################
CLUTO=function(j) {
comando=(paste("vcluster -showfeatures -clabelfile=wordNames.txt cluto_matrix.mat ",j,"  > output_",j,".txt", sep=""))
system(comando)
print(j)
}
J=10
CLUTO(J)
####################################################################################
####################### merging clusters, data and general infos ###################
####################################################################################
options(scipen=999)
data=merge(data,general_infos)
rm(general_infos)
CLUSTERS=read.table(paste("cluto_matrix.mat.clustering.",J,sep=""))
table(CLUSTERS)
############################################################
############ removing the NA rows ##########################
############################################################
table(is.na(data$MESSAGE))
if(sum(is.na(data$MESSAGE))>0) {data=data[-which(is.na(data$MESSAGE)),]}
###################################################################
######## removing useless objects #################################
###################################################################
rm(Uni,taggedResults,ID)
###################################################################
######## removing unclustered observations ########################
###################################################################
if(length(which(data$cluster<0))) data=data[-which(data$cluster<0),]
##################################################################
###################### saving output #############################
##################################################################
#data=apply(data,2,as.character)
colnames(data)=c("ID","tweet","polarity","Profilo_Istituzionale","cluster")
#write.csv2(data,file="textMining_example.csv",row.names=F)
##############################################################################################################
##############################################################################################################
######## R parallelization: k-means example on bivariate randomly generated data #############################
##############################################################################################################
##############################################################################################################
# This tutorial is about R parallelization and has been inspired from http://glennklockwood.com/di/R-para.php
rm(list=ls()) #clear workspace
setwd("/pico/home/userinternal/msartori/Documents/CORSO_MILANO/DATA/") #set working directory
source("/pico/home/userinternal/msartori/Documents/CORSO_MILANO/CODE/R/genData.R") #loading the data generation function
#install.packages("e1071")
# install.packages("doMC")
# install.packages("snow")
# install.packages("foreach")
library(e1071)
library(parallel)
library(foreach)
library(doMC)
library(snow)
# setting the parameters
nrow <- 10000 # number of obs of each cluster
sd <- c(0.5,1,0.7,0.9,0.4) # standard deviation of each cluster
real.centers <- list( x=c(-1.3, -1.5, 0.0, +0.7, +1.1), y=c(-1.0, +1.5, 0.1, -1.3, +1.1) ) # the real centers of the clusters
seed=1234 # set seed: in this way the generated data will be replicable
# data generation
data=genData(nrow,sd,real.centers,seed) # total: 50000 bivariate obs (10000 obs for each group)
labels=data$labels
data=data$data
# plot the data
plot(x=data[,1],y=data[,2],col=labels,main=paste("Randomly generated bivariate data, n =",nrow(data) ),xlab="x",ylab="y")
# shannon entropy of the data
real_entropy=sum((table(labels)/length(labels))*log(table(labels)/length(labels)))
plot(x=data[,1],y=data[,2],col=labels,main=paste("Randomly generated bivariate data, n =",nrow(data) ),xlab="x",ylab="y")
# write and read the data
write.csv(data, file='genData.csv', row.names=FALSE)
data <- read.csv('genData.csv') #load the data set
# removing useless items
rm(real.centers,sd,nrow,seed)
##############################################################################
######## definition of the parallelization function ##########################
##############################################################################
# how many cores do we want to exploit during the parallelization?
ncores=5
parallel.function <- function(i) { kmeans( x=data, centers=5, nstart=i ) }
# NB: the parallelization will not be about splitting the data, but about splitting the multiple starts
# the data are hard-coded in the function, the only variable here is the number of multiple starts
begin=Sys.time()
serial_result <- kmeans( data,  centers=5, nstart=100 ) #no need to use the parallelized function
end=Sys.time()
serial_time=end-begin
print(serial_time)
rm(end,begin)
ls(serial_result)
# matching the results with the actual labels
matchClasses(table(labels,serial_result$cluster),method="exact")
# shannon entropy of the result
serial_entropy=sum((table(serial_result$cluster)/length(serial_result$cluster))*log(table(serial_result$cluster)/length(serial_result$cluster)))
##############################################################################
################## EXAMPLE 3: mclapply (shared-memory parallelism) ###########
##############################################################################
# The "parallel" library provides the mclapply() function which is a parallelized replacement for lapply
# This function distributes the lapply tasks across multiple CPU cores to be executed in parallel
# All of the hard work is in restructuring your problem to use lapply when, serially, it wouldn't necessarily make sense.
# WARNING: it does not work on Windows systems!!!
begin=Sys.time()
results_list <- mclapply( rep(ceiling(100/ncores),times=ncores), FUN=parallel.function , mc.cores=ncores )
tot.withinss <- sapply( results_list, function(results_list) { results_list$tot.withinss } )
mclapply_result <- results_list[[which.min(tot.withinss)]]
end=Sys.time()
mclapply_time=end-begin
print(mclapply_time)
rm(end,begin,results_list,tot.withinss)
ls(mclapply_result)
# matching the results with the actual labels
matchClasses(table(labels,mclapply_result$cluster),method="exact")
# entropy of the result
mclapply_entropy=sum((table(mclapply_result$cluster)/length(mclapply_result$cluster))*log(table(mclapply_result$cluster)/length(mclapply_result$cluster)))
##############################################################################
######## EXAMPLE 5: foreach (shared-memory parallelism) ######################
##############################################################################
registerDoMC(ncores) #it is a necessary step in order to make foreach work in parallel
begin=Sys.time()
results_list <- foreach( i = rep(ceiling(100/ncores),times=ncores) ) %dopar% parallel.function(i)
tot.withinss <- sapply( results_list, function(results_list) { results_list$tot.withinss } )
doMC_result <- results_list[[which.min(tot.withinss)]]
end=Sys.time()
doMC_time=end-begin
print(doMC_time)
rm(end,begin,results_list,tot.withinss)
ls(doMC_result)
# matching the results with the actual labels
matchClasses(table(labels,doMC_result$cluster),method="exact")
# entropy of the result
doMC_entropy=sum((table(doMC_result$cluster)/length(doMC_result$cluster))*log(table(doMC_result$cluster)/length(doMC_result$cluster)))
library(Rmpi)
##############################################################################################################
##############################################################################################################
######## R parallelization: model-based clustering example on bivariate randomly generated data ##############
##############################################################################################################
##############################################################################################################
# This tutorial is about R parallelization and has been inspired from http://glennklockwood.com/di/R-para.php
rm(list=ls()) #clear workspace
setwd("/pico/home/userinternal/msartori/Documents/CORSO_MILANO/DATA/") #set working directory
source("/pico/home/userinternal/msartori/Documents/CORSO_MILANO/CODE/R/genData.R") #loading the data generation function
# install.packages("e1071")
# install.packages("snow")
# install.packages("mclust")
# install.packages("doMC")
# install.packages("foreach")
library(e1071)
library(parallel)
library(foreach)
library(doMC)
library(mclust)
library(snow)
# setting the parameters
nrow <- 1000 # number of obs of each cluster
sd <- c(0.5,1,0.7,0.9,0.4) # standard deviation of each cluster
real.centers <- list( x=c(-1.3, -1.5, 0.0, +0.7, +1.1), y=c(-1.0, +1.5, 0.1, -1.3, +1.1) ) # the real centers of the clusters
seed=1234 # set seed: in this way the generated data will be replicable
#data generation
data=genData(nrow,sd,real.centers,seed) # total: 5000 bivariate obs (1000 obs for each group)
labels=data$labels
data=data$data
# plot the data
plot(x=data[,1],y=data[,2],col=labels,main="Randomly generated bivariate data",xlab="x",ylab="y")
# write and read the data
write.csv(data, file='smallGenData.csv', row.names=FALSE)
data <- read.csv('smallGenData.csv') #load the data set
# removing useless items
rm(real.centers,sd,nrow,seed)
# parameter matrix: each row is a combination of modelNames and G (see Mclust function)
modelNames=c("EII","EEI","EEE","VVV")
G=4:6
param=expand.grid(G=G,modelNames=modelNames) #12 combinations!
rm(G,modelNames)
# parameter matrix: each row is a combination of modelNames and G (see Mclust function)
modelNames=c("EII","EEI","EEE","VVV")
G=4:6
param=expand.grid(G=G,modelNames=modelNames) #12 combinations!
rm(G,modelNames)
param
?getModelNames
?modelNames
# how many cores do we want to exploit during the parallelization?
ncores=ifelse(nrow(param)>16,yes=16,no=nrow(param))
parallel.function <- function(i) { Mclust( data=data, G=as.character(param$G[i]), modelNames=as.character(param$modelNames[i]),prior=priorControl()) }
# NB: the parallelization will not be about splitting the data, but about choosing the best parameters
# the data are hard-coded in the function, the variables here are "G"(1) and "modelNames"(2)
b=Sys.time()
results_list=mclapply((1:nrow(param)),parallel.function,mc.cores=ncores)
e=Sys.time()
mclapply_time=e-b
print(mclapply_time)
mclapply_result=results_list[[which.max(sapply(results_list,function(x){x$bic}))]]
rm(e,b,results_list)
ls(mclapply_result)
print(mclapply_result$G)
print(mclapply_result$modelName)
# matching the results with the actual labels
matchClasses(table(labels,mclapply_result$cl),method="exact")
# entropy of the result
mclapply_entropy=sum((table(mclapply_result$cl)/length(mclapply_result$cl))*log(table(mclapply_result$cl)/length(mclapply_result$cl)))
# which obs have been incorrectly classified?
errors=labels
errors[classError(mclapply_result$cl,truth=labels)$misclassified]=2
errors[-classError(mclapply_result$cl,truth=labels)$misclassified]=1
plot(data$x,data$y,col=errors,main="misclassified obbservations (red dots)",xlab="x",ylab="y")
print(mclapply_result$G)
print(mclapply_result$modelName)
b=Sys.time()
Mclust(data,G=mclapply_result$G,modelNames=mclapply_result$modelName,prior=priorControl())
e=Sys.time()
print(e-b)
rm(b,e)
# running this model only takes approximately the same time of the parallelized function
##############################################################################
######## EXAMPLE 3: foreach (shared-memory parallelism) ######################
##############################################################################
registerDoMC(ncores) #it is a necessary step in order to make foreach work in parallel
b=Sys.time()
results_list <- foreach(i = 1:nrow(param)) %dopar% parallel.function(i)
e=Sys.time()
doMC_result = results_list[[which.max(sapply(results_list,function(x){x$bic}))]]
doMC_time=e-b
print(doMC_time)
rm(e,b,results_list)
ls(doMC_result)
#################################################################################################
######## Supervised modelling with R: the CARET package #########################################
#################################################################################################
# this tutorial has been inspired from:
# http://will-stanton.com/machine-learning-with-r-an-irresponsibly-fast-tutorial/
library(caret)
library(randomForest)
library(rpart)
library(rpart.plot)
library(mclust)
library(e1071)
library(ROCR)
library(foreach)
library(doMC)
setwd("/pico/home/userinternal/msartori/Documents/CORSO_MILANO/DATA/TITANIC") #set working directory
#setwd("~/Google Drive/CINECA/CORSO_PARALLEL_COMPUTING/DATA/TITANIC") #set working directory
rm(list=ls()) #clear workspace
set.seed(123) #set the seed
will use only the training set, since the test set is unlabelled
data=read.csv("train.csv",header=T,sep=",")
?registerDoMC
# it is the best way to keep things simple
data$Embarked[which(data$Embarked=="")]=NA
NAs=function(i)length(which(is.na(data[i,c(1,2,3,5,6,7,8,10,12)])))
remove=(unlist(lapply(1:nrow(data),NAs)))
data=data[which(remove==0),]
rm(remove,NAs)
####### coerce a)Survived (1=yes, 0=no), b)Pclass and c)Embarked to factors ##############
##########################################################################################
data$Pclass=factor(data$Pclass)
data$Survived=factor(data$Survived)
data$Embarked=factor(data$Embarked)
trainID=sample(nrow(data),nrow(data)*(4/5))
trainSet=data[trainID,-c(1,4,9,11)]
testSet=data[-trainID,-c(1,4,9,11)]
# some variables have been excluded!
table(trainSet$Survived,trainSet$Pclass) #it might be an interesting predictor
summary(trainSet$Age)
boxplot(trainSet$Age~trainSet$Survived) #probably not a good predictor
summary(trainSet$Fare) #no NAs
boxplot(trainSet$Fare~trainSet$Survived) #it may be an interesting predictor
table(trainSet$Survived,trainSet$Sex)
table(trainSet$Sex,trainSet$Pclass)
# important feature: complexity parameter (cp) of the tree
# fit the model
dt=rpart(Survived~Pclass+Sex+Age+SibSp+Embarked+Parch+Fare,data=trainSet)
dt
summary(dt)
ls(dt)
rpart.plot(dt,type=4)
dt_pred=predict(dt,newdata=testSet)
pred=prediction(predictions=dt_pred[,2],labels=testSet$Survived,label.ordering=c("0","1"))
perf=performance(pred,"spec","sens")
plot(perf,print.cutoffs.at=c(0.5),colorize=T,main="ROC curve (decision tree)")
rm(perf,pred)
table(data$Survived)
dt_pred=as.factor(apply(dt_pred,1,which.max)-1)
confusionMatrix(dt_pred,testSet$Survived, positive="1")
rf1=randomForest(Survived~Pclass+Sex+Age+SibSp+Embarked+Parch+Fare,data=trainSet,ntree=500)
rf1
ls(rf1)
# visualize a single tree grown by the randomForest algorithm
getTree(rf1,k=1,labelVar=T)
table(is.na(getTree(rf1,k=1,labelVar=T)$pred))
?randomForest
getTree(rf1,k=33,labelVar=T)
rf1_pred=predict(rf1,newdata=testSet)
confusionMatrix(rf1_pred,testSet$Survived, positive="1")
dt_pred=predict(dt,newdata=testSet)
dt_pred
dim(dt_pred)
dim(unique(dt_pred))
# fit the model (with parallelization over the CV step)
begin=Sys.time()
rf2_caret=train(Survived~Pclass+Sex+Age+SibSp+Embarked+Parch+Fare, #model formula
data=trainSet,
method="rf", #method=randomForest
trControl=trainControl(method = "cv",number=20,classProbs=T)) #cross-validation
end=Sys.time()
print(end-begin)
rm(end,begin)
# NB: CV (cross-validation) evaluates the performance of a model using only the training data
# Caret itself does this!!
# workfolow:
# 1) split the training data into (k) equally sized pieces called folds
# 2) train the model on (k-1)/k folds, and check its accuracy on the k-th fold
# 3) repeat this process with each split of the data
# 4) at the end, the percentage accuracy across the different splits of the data is averaged
# view the estimated model
rf2_caret
ls(rf2_caret)
rf2_caret$bestTune
rf2_caret$coef
rf2_caret$finalModel
# predict new obs and confusion table
rf2_pred=predict(rf2_caret,newdata=testSet)
confusionMatrix(rf2_pred,testSet$Survived, positive="1")
dim(trainSet[,-1])
dim(unique(trainSet[,-1]))
# fit the model
Survived=trainSet$Survived=="1"
log1=glm(cbind(Survived,1-Survived)~Pclass+Sex+Age+SibSp+Embarked+Parch+Fare, family=binomial,data=trainSet[,-1])
cbind(Survived,1-Survived)
summary(log1) # Pclass, Sex and Age have significant coefficients
log1_pred=round(predict(log1,newdata=testSet,type="response"))
confusionMatrix(log1_pred,testSet$Survived, positive="1")
pred=prediction(predictions=predict(log1,newdata=testSet,type="response"),labels=testSet$Survived,label.ordering=c("0","1"))
